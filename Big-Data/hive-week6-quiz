1. Column based file formats can specifically be used for: Faster reads and few columns to be processed.
2. Text file format is not considered suitable for production, reason being: Overhead of type-conversion, file size can become bulky, Bytes transferred over network is more
3. Data with schema attached to it. Lots of reads and writes. Which file format is best for it? : Avro, It supports data schemas that change over time.
4. Which file format is suitable with hive? : ORC
5. Which file format is best for schema evolution? : Avro
6. Which file format is best suited for deeply nested data? : Parquet
7. Which file format is the best choice for datalake landing zone? - Avro, data is read as a whole.
8. Row based file formats are efficient at writing and column based file formats are efficient at reading.
9. In order to ensure that the table is loaded only once, which property can be used? : immutable
10. Using the thrift service interface, a remote python client can connect to hive and execute queries. : True
11. Which compression is the fastest among gzip, snapping, lzo, bzip2? : Snappy
12. Which file formats are not splittable? : Snappy & Gzip
13. Which file format is best suitable for archival purposes? - bzip2
14. If you are writing ANDed list of expressions in the WHERE clause, you should keep your UDF on the right.
15. Lzo is not freely available with Hadoop. Gzip and Bzip2 are splittable comopression codecs.
16. If an external partition table is pointed to the HDFS directory and 2 paritions are added manually. Then no paritions will show if you try to do show partitions.
17. Vectorization: It's false by default and it needs to store your data in the Orc format.
18. Thrift service doesn't run on port 2181 by default and it doesn't just use the Java programming language.
19. LZO compression: It's a good choice for text files. LZO files are splittable.
20. Tez and spark execution engines are faster compared to MapReduce and Vectorization and CBO helps us to optimize the hive query.
